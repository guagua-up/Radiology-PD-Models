library(tidymodels)
library(naniar)
library(future)
library(pROC)
library(ggplot2)
library(patchwork)
library(kernlab)

### Data preparation
Wholebrain <- read.csv("D:/PD/Imagings/SUVR/CentrumSemiovale_1/TD_PIGD/TD_PIGD_Wholebrain/MODELS/wb_FinalFeature.csv")
Wholebrain <- Wholebrain[, -c(1:4)] 
Wholebrain[,1] <- factor(Wholebrain[,1], levels = c(1, 0), labels = c("Class1", "Class0"))
names(Wholebrain)[1] <- "Diagnosis"
str(Wholebrain)
Wholebrain |> count(Diagnosis)
miss_var_summary(Wholebrain)

# Data split
set.seed(33)
data_split <- initial_split(data = Wholebrain, prop = 0.8, strata = Diagnosis)
training_data <- analysis(data_split)
testing_data <- assessment(data_split)

resamples <- vfold_cv(data = training_data,
                      v = 10,
                      repeats = 5,
                      strata = Diagnosis)

# ==================== Logistic Regression(LR) ====================
# parsnip
lr_spec <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification")

# Recipe
recipe_lr <- recipe(Diagnosis ~ ., data = training_data) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
lr_wf <- workflow() |>
  add_recipe(recipe_lr) |>
  add_model(lr_spec)

# Fit
lr_fit <- fit(lr_wf, data = training_data)

# Prediction
lr_prediction_tra <- augment(lr_fit, new_data = training_data)
lr_prediction_tes <- augment(lr_fit, new_data = testing_data)

# Calculate metrics
lr_perf <- list(
  tra = list(
    accuracy = accuracy(data = lr_prediction_tra, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = lr_prediction_tra, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = lr_prediction_tra, truth = Diagnosis, .pred_class),
    specificity = spec(data = lr_prediction_tra, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(lr_prediction_tra$Diagnosis) - 1, 
                  predictor = lr_prediction_tra$.pred_Class1)
  ),
  tes = list(
    accuracy = accuracy(data = lr_prediction_tes, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = lr_prediction_tes, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = lr_prediction_tes, truth = Diagnosis, .pred_class),
    specificity = spec(data = lr_prediction_tes, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(lr_prediction_tes$Diagnosis) - 1, 
                  predictor = lr_prediction_tes$.pred_Class1)
  )
)

# ==================== Support Vector Machine (SVM) ====================
# parsnip
svm_spec <- svm_poly(degree = 1) |>
  set_mode("classification") |>
  set_engine("kernlab", scaled = FALSE) |>
  set_args(cost = tune())

# Recipe
recipe_svm <- recipe(Diagnosis ~ ., data = training_data) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
svm_wf <- workflow() |>
  add_recipe(recipe_svm) |>
  add_model(svm_spec)

# Tune Parameters
param_grid <- grid_regular(cost(), levels = 10)

plan(multisession, workers = parallel::detectCores() - 1) 
tune_svm <- tune_grid(
  svm_wf,
  resamples = resamples,
  grid = param_grid,
  metrics = metric_set(roc_auc, f_meas),
  control = control_grid(verbose = TRUE,
                         allow_par = TRUE,
                         parallel_over = "everything",
                         event_level = "first")
)

# Optimal Parameters
best_cost <- tune_svm |> select_best(metric = 'roc_auc')

# Fit Final Model
svm_final <- finalize_workflow(svm_wf, best_cost)
svm_fit <- fit(svm_final, data = training_data)

# Prediction
svm_prediction_tra <- augment(svm_fit, new_data = training_data)
svm_prediction_tes <- augment(svm_fit, new_data = testing_data)

# Calculate metrics
svm_perf <- list(
  tra = list(
    accuracy = accuracy(data = svm_prediction_tra, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = svm_prediction_tra, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = svm_prediction_tra, truth = Diagnosis, .pred_class),
    specificity = spec(data = svm_prediction_tra, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(svm_prediction_tra$Diagnosis) - 1, 
                  predictor = svm_prediction_tra$.pred_Class1)
  ),
  tes = list(
    accuracy = accuracy(data = svm_prediction_tes, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = svm_prediction_tes, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = svm_prediction_tes, truth = Diagnosis, .pred_class),
    specificity = spec(data = svm_prediction_tes, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(svm_prediction_tes$Diagnosis) - 1, 
                  predictor = svm_prediction_tes$.pred_Class1)
  )
)

# ==================== Random Forest (RF) ====================
# parsnip
rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) |>
  set_engine('ranger') |>
  set_mode('classification')

# recipe
recipe_rf <- recipe(Diagnosis ~ ., data = training_data) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
rf_wf <- workflow() |>
  add_recipe(recipe_rf) |>
  add_model(rf_spec)

rf_param <- rf_wf |>
  extract_parameter_set_dials() |>
  update(mtry = mtry_prop(c(0.1, 1)))

# Tune Parameters
tune_rf <- tune_grid(rf_wf,
                     resamples = resamples,
                     param_info = rf_param,
                     grid = grid_space_filling(rf_param, size = 8),
                     metrics = metric_set(roc_auc, f_meas),
                     control = control_grid(verbose = TRUE,
                                            allow_par = TRUE,
                                            parallel_over = "everything",
                                            event_level = "first"))

# Optimal Parameters
optim_rf_param <- tune_rf |> select_best(metric = 'roc_auc')

# Fit Final Model
final_rf <- finalize_workflow(rf_wf, parameters = optim_rf_param)
rf_fit <- fit(final_rf, data = training_data)

# Prediction
rf_prediction_tra <- augment(rf_fit, new_data = training_data)
rf_prediction_tes <- augment(rf_fit, new_data = testing_data)

# Calculate metrics
rf_perf <- list(
  tra = list(
    accuracy = accuracy(data = rf_prediction_tra, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = rf_prediction_tra, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = rf_prediction_tra, truth = Diagnosis, .pred_class),
    specificity = spec(data = rf_prediction_tra, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(rf_prediction_tra$Diagnosis) - 1, 
                  predictor = rf_prediction_tra$.pred_Class1)
  ),
  tes = list(
    accuracy = accuracy(data = rf_prediction_tes, truth = Diagnosis, .pred_class),
    auc = roc_auc(data = rf_prediction_tes, truth = Diagnosis, .pred_Class1),
    sensitivity = sens(data = rf_prediction_tes, truth = Diagnosis, .pred_class),
    specificity = spec(data = rf_prediction_tes, truth = Diagnosis, .pred_class),
    roc_obj = roc(response = as.numeric(rf_prediction_tes$Diagnosis) - 1, 
                  predictor = rf_prediction_tes$.pred_Class1)
  )
)

# ==================== Results summary and visualization ====================
# Plot ROC curve
train_roc_data <- list(
  LR = lr_perf$tra$roc_obj,
  SVM = svm_perf$tra$roc_obj,
  RF = rf_perf$tra$roc_obj
)

test_roc_data <- list(
  LR = lr_perf$tes$roc_obj,
  SVM = svm_perf$tes$roc_obj,
  RF = rf_perf$tes$roc_obj
)

custom_theme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(fill = NA, color = "black"),
  legend.position = c(0.95, 0.05),
  legend.justification = c(1, 0),
  legend.background = element_rect(fill = "white", color = "black"),
  legend.title = element_blank(),
  legend.text = element_text(size = 11, face = "bold"),  
  axis.text = element_text(size = 10),
  axis.title = element_text(size = 12),
  plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
)

train_legend_labels <- c(
  paste0("LR AUC = ", round(auc(lr_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(lr_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(lr_perf$tra$roc_obj)[3], 3), ")"),
  paste0("SVM AUC = ", round(auc(svm_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(svm_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(svm_perf$tra$roc_obj)[3], 3), ")"),
  paste0("RF AUC = ", round(auc(rf_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(rf_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(rf_perf$tra$roc_obj)[3], 3), ")")
)

train_roc_plot <- ggroc(train_roc_data, legacy.axes = TRUE, size = 1) +
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), 
               color = "grey", linetype = "dashed") +
  scale_color_manual(
    values = c("#E41A1C", "#377EB8", "#4DAF4A"),
    labels = train_legend_labels
  ) +
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)",
       title = "ROC Curves (Training Set)") +
  custom_theme +
  guides(color = guide_legend(override.aes = list(size = 2)))

test_legend_labels <- c(
  paste0("LR AUC = ", round(auc(lr_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(lr_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(lr_perf$tes$roc_obj)[3], 3), ")"),
  paste0("SVM AUC = ", round(auc(svm_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(svm_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(svm_perf$tes$roc_obj)[3], 3), ")"),
  paste0("RF AUC = ", round(auc(rf_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(rf_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(rf_perf$tes$roc_obj)[3], 3), ")")
)

test_roc_plot <- ggroc(test_roc_data, legacy.axes = TRUE, size = 1) +
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), 
               color = "grey", linetype = "dashed") +
  scale_color_manual(
    values = c("#E41A1C", "#377EB8", "#4DAF4A"),
    labels = test_legend_labels
  ) +
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)",
       title = "ROC Curves (Test Set)") +
  custom_theme +
  guides(color = guide_legend(override.aes = list(size = 2)))

print(train_roc_plot)
print(test_roc_plot)
ggsave("D:/PD/MODELS/svmlgrf_ROC_Train.png", 
       train_roc_plot, width = 6, height = 6, dpi = 300)
ggsave("D:/PD/MODELS/svmlgrf_ROC_Test.png", 
       test_roc_plot, width = 6, height = 6, dpi = 300)

# Summary metrics
performance_summary <- data.frame(
  Model = rep(c("LR", "SVM", "RF"), each = 2),
  Dataset = rep(c("Training", "Test"), times = 3),
  AUC = c(
    lr_perf$tra$auc$.estimate, lr_perf$tes$auc$.estimate,
    svm_perf$tra$auc$.estimate, svm_perf$tes$auc$.estimate,
    rf_perf$tra$auc$.estimate, rf_perf$tes$auc$.estimate
  ),
  AUC_CI = c(
    paste0(round(ci.auc(lr_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(lr_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(lr_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(lr_perf$tes$roc_obj)[3], 3)),
    paste0(round(ci.auc(svm_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(svm_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(svm_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(svm_perf$tes$roc_obj)[3], 3)),
    paste0(round(ci.auc(rf_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(rf_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(rf_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(rf_perf$tes$roc_obj)[3], 3))
  ),
  Sensitivity = c(
    lr_perf$tra$sensitivity$.estimate, lr_perf$tes$sensitivity$.estimate,
    svm_perf$tra$sensitivity$.estimate, svm_perf$tes$sensitivity$.estimate,
    rf_perf$tra$sensitivity$.estimate, rf_perf$tes$sensitivity$.estimate
  ),
  Specificity = c(
    lr_perf$tra$specificity$.estimate, lr_perf$tes$specificity$.estimate,
    svm_perf$tra$specificity$.estimate, svm_perf$tes$specificity$.estimate,
    rf_perf$tra$specificity$.estimate, rf_perf$tes$specificity$.estimate
  )
)


# ==================== Permutation test ====================
set.seed(1)
perm_test_auc <- function(data, model, n_perm = 1000) {
  actual_auc <- roc_auc(data, truth = Diagnosis, .pred_Class1, event_level = "first")$.estimate
  perm_aucs <- numeric(n_perm)
  
  for (i in 1:n_perm) {
    perm_data <- data %>%
      mutate(perm_Diagnosis = sample(Diagnosis))
    
    perm_auc <- roc_auc(perm_data, truth = perm_Diagnosis, .pred_Class1, event_level = "first")$.estimate
    perm_aucs[i] <- perm_auc
  }
  
  p_value <- mean(perm_aucs >= actual_auc)
  p_value_display <- ifelse(p_value < 0.001, "<0.001", round(p_value, 3))
  
  return(list(actual_auc = actual_auc, perm_aucs = perm_aucs, p_value = p_value, p_value_display = p_value_display))
}

lr_perm_test_tes <- perm_test_auc(lr_prediction_tes, lr_fit)
svm_perm_test_tes <- perm_test_auc(svm_prediction_tes, svm_fit)
rf_perm_test_tes <- perm_test_auc(rf_prediction_tes, rf_fit)

performance_summary <- performance_summary %>%
  mutate(
    AUC_Perm_PValue = c(
      NA, lr_perm_test_tes$p_value_display,
      NA, svm_perm_test_tes$p_value_display,
      NA, rf_perm_test_tes$p_value_display
    )
  )

print("Performance Summary:")
print(performance_summary)

# Confusion matrix
lr_conf_mat_tra <-  conf_mat(data = lr_prediction_tra, truth = Diagnosis, .pred_class)
lr_conf_mat_tes <- conf_mat(data=lr_prediction_tes, truth = Diagnosis, .pred_class)
svm_conf_mat_tra <- conf_mat(data=svm_prediction_tra, truth = Diagnosis, .pred_class)
svm_conf_mat_tes <- conf_mat(data=svm_prediction_tes, truth = Diagnosis, .pred_class)
rf_conf_mat_tra <- conf_mat(data=rf_prediction_tra, truth = Diagnosis, .pred_class)
rf_conf_mat_tes <- conf_mat(data=rf_prediction_tes, truth = Diagnosis, .pred_class)
lr_conf_mat_tra
lr_conf_mat_tes
svm_conf_mat_tra
svm_conf_mat_tes
rf_conf_mat_tra
rf_conf_mat_tes
