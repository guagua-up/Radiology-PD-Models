library(tidymodels)
library(naniar)
library(future)
library(pROC)
library(ggplot2)
library(patchwork)
library(vip)

# ==================== WB Model ====================
# Data preparation
Wholebrain <- read.csv("D:/PD/LASSO/wb_FinalFeature.csv")
Wholebrain <- Wholebrain[, -c(1:4)] 
Wholebrain[,1] <- factor(Wholebrain[,1], levels = c(0, 1), labels = c("Class0", "Class1"))
names(Wholebrain)[1] <- "Diagnosis"
str(Wholebrain)
Wholebrain |> count(Diagnosis)
miss_var_summary(Wholebrain)

# Data split
set.seed(985)
data_split <- initial_split(data = Wholebrain, prop = 0.8, strata = Diagnosis)
training_data <- analysis(data_split)
testing_data <- assessment(data_split)

# parsnip
rf_spec <- rand_forest(mtry = tune(),
                       trees = tune(),
                       min_n = tune()) |>
  set_engine('ranger', importance = "permutation") |>
  set_mode('classification')

# recipe
recipe <- recipe(Diagnosis ~ ., data = training_data) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
rf <- workflow() |>
  add_recipe(recipe) |>
  add_model(rf_spec)

rf_param <- rf |>
  extract_parameter_set_dials() |>
  update(mtry = mtry_prop(c(0.1, 1)))

# Resamples
resamples <- vfold_cv(data = training_data,
                      v = 10,
                      repeats = 5,
                      strata = Diagnosis)

# Tune Parameters
plan(multisession, workers = parallel::detectCores() - 1) 
tune_rf <- tune_grid(rf,
                     resamples = resamples,
                     param_info = rf_param,
                     grid = grid_space_filling(rf_param, size = 8),
                     metrics = metric_set(roc_auc, f_meas),
                     control = control_grid(verbose = TRUE,
                                            allow_par = TRUE,
                                            parallel_over = "everything",
                                            event_level = "second"))

# Optimal Parameters
optim_rf_param <- tune_rf |> select_best(metric = 'roc_auc')

# Fit Final Model
final_rf <- finalize_workflow(rf, parameters = optim_rf_param)
rf_fit <- fit(final_rf, data = training_data)

# Prediction
prediction_tra <- augment(rf_fit, new_data = training_data)
prediction_tes <- augment(rf_fit, new_data = testing_data)

# Calculate metrics
wb_perf <- list(
  tra = list(
    accuracy = accuracy(data = prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = prediction_tra, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(prediction_tra$Diagnosis) - 1, 
                  predictor = prediction_tra$.pred_Class1)
  ),
  tes = list(
    accuracy = accuracy(data = prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = prediction_tes, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(prediction_tes$Diagnosis) - 1, 
                  predictor = prediction_tes$.pred_Class1)
  )
)

# ==================== NS Model ====================
# Data preparation
NS <- read.csv("D:/PD/LASSO/ns_FinalFeature.csv")
NS <- NS[, -c(1:4)]
NS[,1] <- factor(NS[,1], levels = c(0, 1), labels = c("Class0", "Class1"))
names(NS)[1] <- "Diagnosis"
str(NS)
NS |> count(Diagnosis)
miss_var_summary(NS)

# Data split
set.seed(985)
split_ns <- initial_split(data = NS, prop = 0.8, strata = Diagnosis)
training_data_ns <- analysis(split_ns)
testing_data_ns <- assessment(split_ns)

# parsnip
rf_ns <- rand_forest(mtry = tune(),
                     trees = tune(),
                     min_n = tune()) |>
  set_engine('ranger', importance = "permutation") |>
  set_mode('classification')

# recipe
recipe_ns <- recipe(Diagnosis ~ ., data = training_data_ns) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
rf_ns <- workflow() |>
  add_recipe(recipe_ns) |>
  add_model(rf_ns)

rf_param_ns <- rf_ns |>
  extract_parameter_set_dials() |>
  update(mtry = mtry_prop(c(0.1, 1)))

# Resamples
resamples <- vfold_cv(data = training_data_ns,
                      v = 10,
                      repeats = 5,
                      strata = Diagnosis)

# Tune Parameters
plan(multisession, workers = parallel::detectCores() - 1) 
tune_rf_ns <- tune_grid(rf_ns,
                        resamples = resamples,
                        param_info = rf_param_ns,
                        grid = grid_space_filling(rf_param_ns, size = 8),
                        metrics = metric_set(roc_auc, f_meas),
                        control = control_grid(verbose = TRUE,
                                               allow_par = TRUE,
                                               parallel_over = "everything",
                                               event_level = "second"))

# Optimal Parameters
optim_rf_param_ns <- tune_rf_ns |> select_best(metric = 'roc_auc')

# Fit Final Model
final_rf_ns <- finalize_workflow(rf_ns, parameters = optim_rf_param_ns)
rf_fit_ns <- fit(final_rf_ns, data = training_data_ns)

# Prediction
ns_prediction_tra <- augment(rf_fit_ns, new_data = training_data_ns)
ns_prediction_tes <- augment(rf_fit_ns, new_data = testing_data_ns)

# Calculate metrics
ns_perf <- list(
  tra = list(
    accuracy = accuracy(data = ns_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = ns_prediction_tra, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = ns_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = ns_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(ns_prediction_tra$Diagnosis) - 1, 
                  predictor = ns_prediction_tra$.pred_Class1)
  ),
   tes = list(
    accuracy = accuracy(data = ns_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = ns_prediction_tes, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = ns_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = ns_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(ns_prediction_tes$Diagnosis) - 1, 
                  predictor = ns_prediction_tes$.pred_Class1)
  )
)

# ==================== ONS Model====================
# Data preparation
ONS <- read.csv("D:/PD/LASSO/ons_FinalFeature.csv")
ONS <- ONS[, -c(1:4)]
ONS[,1] <- factor(ONS[,1], levels = c(0, 1), labels = c("Class0", "Class1"))
names(ONS)[1] <- "Diagnosis"
str(ONS)
ONS |> count(Diagnosis)
miss_var_summary(ONS)

# Data split
set.seed(985)
split_ons <- initial_split(data = ONS, prop = 0.8, strata = Diagnosis)
training_data_ons <- analysis(split_ons)
testing_data_ons <- assessment(split_ons)

# parsnip
rf_ons <- rand_forest(mtry = tune(),
                      trees = tune(),
                      min_n = tune()) |>
  set_engine('ranger', importance = "permutation") |>
  set_mode('classification')

# recipe
recipe_ons <- recipe(Diagnosis ~ ., data = training_data_ons) |>
  step_scale(all_numeric_predictors()) |>  
  step_center(all_numeric_predictors())

# workflow
rf_ons <- workflow() |>
  add_recipe(recipe_ons) |>
  add_model(rf_ons)

rf_param_ons <- rf_ons |>
  extract_parameter_set_dials() |>
  update(mtry = mtry_prop(c(0.1, 1)))

# Resamples
resamples <- vfold_cv(data = training_data_ons,
                      v = 10,
                      repeats = 5,
                      strata = Diagnosis)

# Tune Parameters
tune_rf_ons <- tune_grid(rf_ons,
                         resamples = resamples,
                         param_info = rf_param_ons,
                         grid = grid_space_filling(rf_param_ons, size = 8),
                         metrics = metric_set(roc_auc, f_meas),
                         control = control_grid(verbose = TRUE,
                                                allow_par = TRUE,
                                                parallel_over = "everything",
                                                event_level = "second"))

# Optimal Parameters
optim_rf_param_ons <- tune_rf_ons |> select_best(metric = 'roc_auc')

# Fit Final Model
final_rf_ons <- finalize_workflow(rf_ons, parameters = optim_rf_param_ons)
rf_fit_ons <- fit(final_rf_ons, data = training_data_ons)

# Prediction
ons_prediction_tra <- augment(rf_fit_ons, new_data = training_data_ons)
ons_prediction_tes <- augment(rf_fit_ons, new_data = testing_data_ons)

# Calculate metrics
ons_perf <- list(
  tra = list(
    accuracy = accuracy(data = ons_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = ons_prediction_tra, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = ons_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = ons_prediction_tra, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(ons_prediction_tra$Diagnosis) - 1, 
                  predictor = ons_prediction_tra$.pred_Class1)
  ),
  tes = list(
    accuracy = accuracy(data = ons_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    auc = roc_auc(data = ons_prediction_tes, truth = Diagnosis, .pred_Class1, event_level = "second"),
    sensitivity = sens(data = ons_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    specificity = spec(data = ons_prediction_tes, truth = Diagnosis, .pred_class, event_level = "second"),
    roc_obj = roc(response = as.numeric(ons_prediction_tes$Diagnosis) - 1, 
                  predictor = ons_prediction_tes$.pred_Class1)
  )
)

# ==================== Results summary and visualization ====================
# Plot ROC curve
train_roc_data <- list(
  NS = ns_perf$tra$roc_obj,
  ONS = ons_perf$tra$roc_obj,
  WB = wb_perf$tra$roc_obj
)

test_roc_data <- list(
  NS = ns_perf$tes$roc_obj,
  ONS = ons_perf$tes$roc_obj,
  WB = wb_perf$tes$roc_obj
)

custom_theme <- theme(
  panel.background = element_blank(),
  panel.border = element_rect(fill = NA, color = "black"),
  legend.position = c(0.95, 0.05),  
  legend.justification = c(1, 0),    
  legend.background = element_rect(fill = "white", color = "black"),
  legend.title = element_blank(),
  legend.text = element_text(size = 11, face = "bold"),  
  axis.text = element_text(size = 10),
  axis.title = element_text(size = 12),
  plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
)

train_legend_labels <- c(
  paste0("NS AUC = ", round(auc(ns_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(ns_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(ns_perf$tra$roc_obj)[3], 3), ")"),
  paste0("ONS AUC = ", round(auc(ons_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(ons_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(ons_perf$tra$roc_obj)[3], 3), ")"),
  paste0("WB AUC = ", round(auc(wb_perf$tra$roc_obj), 3), 
         " (", round(ci.auc(wb_perf$tra$roc_obj)[1], 3), "-", 
         round(ci.auc(wb_perf$tra$roc_obj)[3], 3), ")")
)

train_roc_plot <- ggroc(train_roc_data, legacy.axes = TRUE, size = 1) +
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), 
               color = "grey", linetype = "dashed") +
  scale_color_manual(
    values = c("#E41A1C", "#377EB8", "#4DAF4A"),
    labels = train_legend_labels
  ) +
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)",
       title = "ROC Curves (Training Set)") +
  custom_theme +
  guides(color = guide_legend(override.aes = list(size = 2)))

test_legend_labels <- c(
  paste0("NS AUC = ", round(auc(ns_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(ns_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(ns_perf$tes$roc_obj)[3], 3), ")"),
  paste0("ONS AUC = ", round(auc(ons_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(ons_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(ons_perf$tes$roc_obj)[3], 3), ")"),
  paste0("WB AUC = ", round(auc(wb_perf$tes$roc_obj), 3), 
         " (", round(ci.auc(wb_perf$tes$roc_obj)[1], 3), "-", 
         round(ci.auc(wb_perf$tes$roc_obj)[3], 3), ")")
)

test_roc_plot <- ggroc(test_roc_data, legacy.axes = TRUE, size = 1.5) +
  geom_segment(aes(x = 0, xend = 1, y = 0, yend = 1), 
               color = "grey", linetype = "dashed") +
  scale_color_manual(values = c("#E41A1C", "#377EB8", "#4DAF4A"), 
                     labels = test_legend_labels) +
  labs(x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)",
       title = "ROC Curves (Test Set)") +
  custom_theme +
  guides(color = guide_legend(override.aes = list(size = 2)))

print(train_roc_plot)
print(test_roc_plot)

ggsave("D:/PD/MODELS/ROC_Train.png", 
       train_roc_plot, width = 6, height = 6, dpi = 300)
ggsave("D:/PD/MODELS/ROC_Test.png", 
       test_roc_plot, width = 6, height = 6, dpi = 300)

# Summary metrics
performance_summary <- data.frame(
  Model = rep(c("WB", "NS", "ONS"), each = 2),
  Dataset = rep(c("Training", "Test"), times = 3),
  AUC = c(
    wb_perf$tra$auc$.estimate, wb_perf$tes$auc$.estimate,
    ns_perf$tra$auc$.estimate, ns_perf$tes$auc$.estimate,
    ons_perf$tra$auc$.estimate, ons_perf$tes$auc$.estimate
  ),
  AUC_CI = c(
    paste0(round(ci.auc(wb_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(wb_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(wb_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(wb_perf$tes$roc_obj)[3], 3)),
    paste0(round(ci.auc(ns_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(ns_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(ns_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(ns_perf$tes$roc_obj)[3], 3)),
    paste0(round(ci.auc(ons_perf$tra$roc_obj)[1], 3), "-", round(ci.auc(ons_perf$tra$roc_obj)[3], 3)),
    paste0(round(ci.auc(ons_perf$tes$roc_obj)[1], 3), "-", round(ci.auc(ons_perf$tes$roc_obj)[3], 3))
  ),
  Sensitivity = c(
    wb_perf$tra$sensitivity$.estimate, wb_perf$tes$sensitivity$.estimate,
    ns_perf$tra$sensitivity$.estimate, ns_perf$tes$sensitivity$.estimate,
    ons_perf$tra$sensitivity$.estimate, ons_perf$tes$sensitivity$.estimate
  ),
  Specificity = c(
    wb_perf$tra$specificity$.estimate, wb_perf$tes$specificity$.estimate,
    ns_perf$tra$specificity$.estimate, ns_perf$tes$specificity$.estimate,
    ons_perf$tra$specificity$.estimate, ons_perf$tes$specificity$.estimate
  )
)

# ==================== Permutation test ====================
set.seed(1)
perm_test_auc <- function(data, model, n_perm = 1000) {
  actual_auc <- roc_auc(data, truth = Diagnosis, .pred_Class1, event_level = "second")$.estimate
  perm_aucs <- numeric(n_perm)
  
  for (i in 1:n_perm) {
    perm_data <- data %>%
      mutate(perm_Diagnosis = sample(Diagnosis))

    perm_auc <- roc_auc(perm_data, truth = perm_Diagnosis, .pred_Class1, event_level = "second")$.estimate
    perm_aucs[i] <- perm_auc
  }
  
  p_value <- mean(perm_aucs >= actual_auc)
  p_value_display <- ifelse(p_value < 0.001, "<0.001", round(p_value, 3))
  
  return(list(actual_auc = actual_auc, perm_aucs = perm_aucs, p_value = p_value, p_value_display = p_value_display))
}

wb_perm_test_tes <- perm_test_auc(prediction_tes, rf_fit)
ns_perm_test_tes <- perm_test_auc(ns_prediction_tes, rf_fit_ns)
ons_perm_test_tes <- perm_test_auc(ons_prediction_tes, rf_fit_ons)

performance_summary <- performance_summary %>%
  mutate(
    AUC_Perm_PValue = c(
      NA, wb_perm_test_tes$p_value_display,
      NA, ns_perm_test_tes$p_value_display,
      NA, ons_perm_test_tes$p_value_display
    )
  )

print("Performance Summary with Permutation Test Results:")
print(performance_summary)

# Confusion matrix
wb_conf_mat_tra <- conf_mat(data=prediction_tra, truth = Diagnosis, .pred_class)
wb_conf_mat_tes <- conf_mat(data=prediction_tes, truth = Diagnosis, .pred_class)
ns_wb_conf_mat_tra <- conf_mat(data=ns_prediction_tra, truth = Diagnosis, .pred_class)
ns_wb_conf_mat_tes <- conf_mat(data=ns_prediction_tes, truth = Diagnosis, .pred_class)
ons_wb_conf_mat_tra <- conf_mat(data=ons_prediction_tra, truth = Diagnosis, .pred_class)
ons_wb_conf_mat_tes <- conf_mat(data=ons_prediction_tes, truth = Diagnosis, .pred_class)
wb_conf_mat_tra
wb_conf_mat_tes
ns_wb_conf_mat_tra
ns_wb_conf_mat_tes
ons_wb_conf_mat_tra
ons_wb_conf_mat_tes

# ==================== Feature Importance ====================
get_imp_data <- function(model_fit, top_n = 20) {
  model_fit |> 
    extract_fit_parsnip() |> 
    vi() |> 
    arrange(desc(Importance)) |> 
    head(top_n)
}

imp_data_wb <- get_imp_data(rf_fit)
imp_data_ns <- get_imp_data(rf_fit_ns)
imp_data_ons <- get_imp_data(rf_fit_ons)

create_imp_plot <- function(imp_data, title, color = "steelblue") {
  ggplot(imp_data, aes(x = reorder(Variable, Importance), y = Importance)) +
    geom_col(fill = color, width = 0.6, alpha = 0.8) +
    labs(title = title, x = "", y = "Importance") +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank(),
      axis.text.y = element_text(color = "black", size = 10),
      axis.text.x = element_text(color = "black", size = 10),
      axis.title.x = element_text(size = 12, margin = margin(t = 10))
    ) +
    coord_flip()
}

wb_imp_plot <- create_imp_plot(imp_data_wb, 
                               "Feature Importance",
                               color = "#377EB8") 

ns_imp_plot <- create_imp_plot(imp_data_ns,
                               "Feature Importance",
                               color = "#377EB8")  

ons_imp_plot <- create_imp_plot(imp_data_ons,
                                "Feature Importance",
                                color = "#377EB8")  
print(wb_imp_plot)
print(ns_imp_plot)
print(ons_imp_plot)

ggsave("D:/PD/MODELS/Feature_Importance_WB.png", 
       wb_imp_plot, width = 6, height = 8, dpi = 300)
ggsave("D:/PD/MODELS/Feature_Importance_ONS.png", 
       wb_imp_plot, width = 6, height = 8, dpi = 300)
ggsave("D:/PD/MODELS/Feature_Importance_NS.png", 
       wb_imp_plot, width = 6, height = 8, dpi = 300)
